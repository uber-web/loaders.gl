{"pageContext":{"isCreatedByStatefulCreatePages":false,"data":[{"excerpt":"What's Next for Apache Arrow in Javascript There are a lot of features we'd like to add over the next few Javascript releases: Inline…","rawMarkdownBody":"# What's Next for Apache Arrow in Javascript\n\nThere are a lot of features we'd like to add over the next few Javascript releases:\n\n* **Inline predicates**: Function calls in the inner loop of a scan over millions of records can be very expensive. We can potentially save that time by generating a new scan function with the predicates inlined when a filter is created.\n\n* **Cache filter results**: Right now every time we do a scan on a filtered DataFrame we re-check the predicate on every row. There should be an (optional?) lazily computed index to store the predicate results for subsequent re-use.\n\n* **Friendlier API**: I shouldn't have to write a custom scan function just to take a look at the results of a filter! Every DataFrame should have a toJSON() function (See ARROW-2202).\n\n* **node.js ↔ (Python, C++, Java, ...) interaction**: A big benefit of Arrow's common in-memory format is that different tools can operate on the same memory. Unfortunately we're pretty closed off in the browser, but node doesn't have that problem! Finishing ARROW-1700, node.js Plasma store client should make this type of interaction possible.\n\nHave an idea? Tell us! Generally JIRAs are preferred but we'll take GitHub issues too. If you just want to discuss something, reach out on the mailing list or slack. But PRs are the best of all, we can always use more contributors!\n\n- **Tensors** - Not yet implemented.\n\n","slug":"docs/roadmap","title":"What's Next for Apache Arrow in Javascript"},{"excerpt":"Arrow JavaScript API The Arrow JavaScript API is designed to helps applications tap into the full power of working with Apache Arrow…","rawMarkdownBody":"# Arrow JavaScript API\n\nThe Arrow JavaScript API is designed to helps applications tap into the full power of working with Apache Arrow formatted data. It supports typical use cases such as loading, processing, generating and writing Arrow encoded data, as well as doing data frame style manipulations on Arrow data.\n\n\n## Resources\n\n* [Apache Arrow Home](https://arrow.apache.org/)\n* [Apache Arrow JS on github](https://github.com/apache/arrow/tree/master/js)\n* [Apache Arros JS on npm](https://www.npmjs.com/package/apache-arrow)\n","slug":"docs","title":"Arrow JavaScript API"},{"excerpt":"What's New v0.4.0 ... v0.3.0 ...","rawMarkdownBody":"# What's New\n\n# v0.4.0\n\n...\n\n\n# v0.3.0\n\n...\n\n","slug":"docs/whats-new","title":"What's New"},{"excerpt":"Notes on Documentation Markdown vs JSDoc Since the Arrow JavaScript API includes both manually written markdown and \"automatically…","rawMarkdownBody":"# Notes on Documentation\n\n\n## Markdown vs JSDoc\n\nSince the Arrow JavaScript API includes both manually written markdown and \"automatically\" generated jsdoc is used for the JS API. Each provides.\n\n- The markdown version focuses on readability.\n- The jsdoc version includes Typescript type information and is more richly hyperlinked.\n\nIn general, to avoid excessive duplication and possible divergence, it is recommended that the JSDoc version contains brief summary texts only.\n\n\n## Updating Docs\n\nIn general, the markdown docs should be considered a (the?) source of truth for the JavaScript API:\n\n* Reviewers should make sure that PRs affecting the JS API code contain appropriate changes to the markdown docs (in the same way that such PRs must contain appropriate changes to e.g. test cases).\n* Bugs should be reviewed first towards the documentation, to see if the documented behavior needs to be changed, and then towards the code.\n","slug":"docs/notes","title":"Notes on Documentation"},{"excerpt":"Working with BigInts If the platform supports   Arrow will use this type, and for convenience inject additional methods (on the object…","rawMarkdownBody":"# Working with BigInts\n\nIf the platform supports `BigInt64Array` Arrow will use this type, and for convenience inject additional methods (on the object instance:\n\n### toJSON(this: BN<BigNumArray>) { return `\"${bignumToString(this)}\"`; },\n### toString(this: BN<BigNumArray>) { return bignumToString(this); },\n### valueOf(this: BN<BigNumArray>) { return bignumToNumber(this); },\n### [Symbol.toPrimitive]<T extends BN<BigNumArray>>(this: T, hint: 'string' | 'number' | 'default') {\n","slug":"docs/developer-guide/big-ints","title":"Working with BigInts"},{"excerpt":"Extracting Data While keeping data in Arrow format allows for efficient data frame operations, there are of course cases where data needs to…","rawMarkdownBody":"# Extracting Data\n\nWhile keeping data in Arrow format allows for efficient data frame operations, there are of course cases where data needs to be extracted in a form that can be use with non-Arrow aware JavaScript code.\n\nMany arrow classes support the following methods:\n\n* `toArray()`\n* `toJSON()`\n* `toString()`\n\n\n### Extracting Data by Row\n\n```js\nconst row = table.get(0);\n```\n\nThe row does not retain the schema, so you'll either need to know the order of columns `row.get(0)`, or use the `to*()` methods.\n\n### Extracting Data by Column\n\n```js\nconst column = table.getColumn('data');\n```\n\n### Extracting data by Column and Batch\n\nA more efficient way to get access to data (especially if the table has not been sliced or filtered)\n\n\n### Extracting predicates from a DataFrame\n\nTo do data frame filtering separately (e.g. on GPU), the frame's predicates need to be extracted:\n\nTBA\n","slug":"docs/developer-guide/converting-data","title":"Extracting Data"},{"excerpt":"Introduction This page provides an overview of the Apache Arrow JS API to help you get started. About Apache Arrow Apache Arrow is a…","rawMarkdownBody":"# Introduction\n\nThis page provides an overview of the Apache Arrow JS API to help you get started.\n\n\n## About Apache Arrow\n\nApache Arrow is a columnar memory layout specification for encoding vectors and table-like containers of flat and nested data. The Arrow spec aligns columnar data in memory to minimize cache misses and take advantage of the latest SIMD (Single input multiple data) and GPU operations on modern processors.\n\nApache Arrow is the emerging standard for large in-memory columnar data (Spark, Pandas, Drill, Graphistry, ...). By standardizing on a common binary interchange format, big data systems can reduce the costs and friction associated with cross-system communication.\n\n\n## Getting Started\n\nTo install and start coding with Apache Arrow JS bindings, see the [Getting Started](docs-arrow/developers-guide/getting-started.md).\n\n\n## Feature Completeness\n\nIdeally each Apache Arrow language binding would offer the same set of features, at least to the extent that the language/platform in question allows. In practice however, not all features have been implemented in all language bindings.\n\nIn comparison with the C++ Arrow API bindings, there are some missing features in the JavaScript bindings:\n\n- Tensors are not yet supported.\n- No explicit support for Apache Arrow Flight\n\n\n## API Design Notes\n\nUnderstanding some of the design decisions made when defining the JavaScript binding API may help facilitate a better appreciateion of why the API is designed the way it is:\n\n- To facilitate the evolution of the bindings, the JavaScript Arrow API is designed to be close match to the C++ Arrow API, although some differences have been made where it makes sense. Some design patterns, like the way `RecordBatchReader.from()` returns different `RecordBatchReader` subclasses depending on what source is being read.\n- To help ensure correctness, the JavaScript arrow binding implementation makes rigorous use of type definitions (through Typescript syntax). In some cases, special methods are provided to ensure that type information \"flows\" correctly from function/constructor arguments to returned objects. The methods `Table.new()` (as an alternative to `new Table()`) is an example of this, that you may want to leverage if your application is written in Typescript.\n\n\n## Resources\n\nThere are some excellent resources available that can help you quickly get a feel for what the type of capabilities the Arrow JS API offers:\n\n* Observable: [Introduction to Apache Arrow](https://observablehq.com/@theneuralbit/introduction-to-apache-arrow)\n* Observable: [Using Apache Arrow JS with Large Datasets](https://observablehq.com/@theneuralbit/using-apache-arrow-js-with-large-datasets)\n* Observable: [Manipulating Flat Arrays, Arrow-Style](https://observablehq.com/@lmeyerov/manipulating-flat-arrays-arrow-style)\n* [Manipulating Flat Arrays](https://observablehq.com/@mbostock/manipulating-flat-arrays) General article on Columnar Data and Data Frames\n\n\n","slug":"docs/developer-guide","title":"Introduction"},{"excerpt":"Data Sources and Sinks The Arrow JavaScript API is designed to make it easy to work with data sources both in the browser and in Node.js…","rawMarkdownBody":"# Data Sources and Sinks\n\nThe Arrow JavaScript API is designed to make it easy to work with data sources both in the browser and in Node.js.\n\n\n## Streams\n\nBoth Node and DOM/WhatWG Streams can be used directly\n\n## Fetch Responses\n\nFetch responses (Promises) can be used where a data source is expected.\n\n## ArrayBuffers\n\nMost data sources accept `Uint8Arrays`.\n\n## AsyncIterators\n\nAsync iterators are the most general way to abstract \"streaming\" data sources and data sinks and are consistently accepted (and in many cased returned) by the Arrow JS API.\n","slug":"docs/developer-guide/data-sources","title":"Data Sources and Sinks"},{"excerpt":"Data Types Arrow supports a rich set of data types: Fixed-length primitive types: numbers, booleans, date and times, fixed size binary…","rawMarkdownBody":"# Data Types\n\nArrow supports a rich set of data types:\n\n* Fixed-length primitive types: numbers, booleans, date and times, fixed size binary, decimals, and other values that fit into a given number\n* Variable-length primitive types: binary, string\n* Nested types: list, struct, and union\n* Dictionary type: An encoded categorical type\n\n\n### Converting Dates\n\nApache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int.\n\n```js\nfunction toDate(timestamp) {\n  return new Date((timestamp[1] * Math.pow(2, 32) + timestamp[0])/1000);\n}\n```\n\n","slug":"docs/developer-guide/data-types","title":"Data Types"},{"excerpt":"Getting Started Installing Apache Arrow The Apache Arrow JS bindings are published as an npm module. or Usage Examples Get a table from an…","rawMarkdownBody":"# Getting Started\n\n\n## Installing Apache Arrow\n\nThe Apache Arrow JS bindings are published as an npm module.\n\n```sh\nnpm install apache-arrow\n```\nor\n```sh\nyarn add apache-arrow\n```\n\n\n## Usage Examples\n\n\n### Get a table from an Arrow file on disk (in IPC format)\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst arrow = readFileSync('simple.arrow');\nconst table = Table.from([arrow]);\n\nconsole.log(table.toString());\n\n/*\n foo,  bar,  baz\n   1,    1,   aa\nnull, null, null\n   3, null, null\n   4,    4,  bbb\n   5,    5, cccc\n*/\n```\n\n### Create a Table when the Arrow file is split across buffers\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst table = Table.from([\n    'latlong/schema.arrow',\n    'latlong/records.arrow'\n].map((file) => readFileSync(file)));\n\nconsole.log(table.toString());\n\n/*\n        origin_lat,         origin_lon\n35.393089294433594,  -97.6007308959961\n35.393089294433594,  -97.6007308959961\n35.393089294433594,  -97.6007308959961\n29.533695220947266, -98.46977996826172\n29.533695220947266, -98.46977996826172\n*/\n```\n\n### Create a Table from JavaScript arrays\n\n```js\nimport {\n  Table,\n  FloatVector,\n  DateVector\n} from 'apache-arrow';\n\nconst LENGTH = 2000;\n\nconst rainAmounts = Float32Array.from(\n  { length: LENGTH },\n  () => Number((Math.random() * 20).toFixed(1)));\n\nconst rainDates = Array.from(\n  { length: LENGTH },\n  (_, i) => new Date(Date.now() - 1000 * 60 * 60 * 24 * i));\n\nconst rainfall = Table.new(\n  [FloatVector.from(rainAmounts), DateVector.from(rainDates)],\n  ['precipitation', 'date']\n);\n```\n\n### Load data with `fetch`\n\n```js\nimport { Table } from \"apache-arrow\";\n\nconst table = await Table.from(fetch((\"/simple.arrow\")));\nconsole.log(table.toString());\n\n```\n\n### Columns look like JS Arrays\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst table = Table.from([\n    'latlong/schema.arrow',\n    'latlong/records.arrow'\n].map(readFileSync));\n\nconst column = table.getColumn('origin_lat');\n\n// Copy the data into a TypedArray\nconst typed = column.toArray();\nassert(typed instanceof Float32Array);\n\nfor (let i = -1, n = column.length; ++i < n;) {\n    assert(column.get(i) === typed[i]);\n}\n```\n","slug":"docs/developer-guide/getting-started","title":"Getting Started"},{"excerpt":"Using Predicates The Arrow API provides standard predicates that allow for the comparison of column values against literals (equality…","rawMarkdownBody":"# Using Predicates\n\n\nThe Arrow API provides standard predicates that allow for the comparison of column values against literals (equality, greater or equal than, less or eqial than) as well as the creation of composite logical expressions (`and`, `or` and `not`) out of individual column comparisons.\n\nIt is of course also possible to write custom predicates, however the performance is best when using the built-ins. Note that for performance reasons, filters are specified using \"predicates\" rather than custom JavaScript functions. For details on available predicates see [Using Predicates]().\n\n## Filtering using Predicates\n\n> Note that calling `filter()` on a `DataFrame` doesn't actually do anything (other than store the predicates). It's not until you call `countBy()` or `scan()` on the resulting object that Arrow actually scans through all of the data.\n\n\n```js\ntable = table.filter(arrow.predicate.col('winnername').eq(winner));\n```\n","slug":"docs/developer-guide/predicates","title":"Using Predicates"},{"excerpt":"Notes on Memory Management Arrow reads in arrow data as arraybuffer(s) and then creates chunks that are \"sub array views\" into that big…","rawMarkdownBody":"# Notes on Memory Management\n\n* Arrow reads in arrow data as arraybuffer(s) and then creates chunks that are \"sub array views\" into that big array buffer, and lists of those chunks are then composed into \"logical\" arrays.\n* Chunks are created for each column in each RecordBatch.\n* The chunks can be \"sliced and diced\" by operations on `Column`, `Table` and `DataFrame` objects, but are never copied (as long as flattening is not requested) and are conceptually immutable. (There is a low-level `Vector.set()` method however given that it could modify data that is used by multiple objects its use should be reserved for cases where implications are fully understood).\n","slug":"docs/developer-guide/memory-management","title":"Notes on Memory Management"},{"excerpt":"Working with Tables References: Much of the text in this section is adapted from Brian Hulette's  Using Apache Arrow JS with Large Datasets…","rawMarkdownBody":"# Working with Tables\n\nReferences:\n* Much of the text in this section is adapted from Brian Hulette's [Using Apache Arrow JS with Large Datasets](https://observablehq.com/@theneuralbit/using-apache-arrow-js-with-large-datasets)\n\n\n## Loading Arrow Data\n\nApplications often start with loading some Arrow formatted data. The Arrow API provides several ways to do this, but in many cases, the simplest approach is to use `Table.from()`.\n\n```js\nimport {Table} from 'apache-arrow';\nconst response = await fetch(dataUrl);\nconst arrayBuffer = await response.arrayBuffer();\nconst dataTable = arrow.Table.from(new Uint8Array(arrayBuffer));\n```\n\n## Getting Records Count\n\n```js\nconst count = table.count();\n```\n\n### Getting Arrow Schema Metadata\n\n```js\nconst fieldNames = table.schema.fields.map(f => f.name);\n// Array(3) [\"Latitude\", \"Longitude\", \"Date\"]\n```\n\n```js\nconst fieldTypes = tables.schema.fields.map(f => f.type)\n// Array(3) [Float, Float, Timestamp]\n\nconst fieldTypeNames = \n// Array(3) [\"Float64\", \"Float64\", \"Timestamp<MICROSECOND>\"]\n\n\n### Accessing Arrow Table Row Data\n\n```js\nconst firstRow = tables.get(0) // 1st row data\nconst lastRow = tables.get(rowCount-1)\n```\n\n## Record toJSON and toArray\n\nIt is easy to converting Rows to JSON/Arrays/Strings:\n\n```js\ntoJSON = Array(3) [41.890751259, -87.71617311899999, Int32Array(2)]\ntoArray = Array(3) [41.933659084, -87.72369064600001, Int32Array(2)]\n```\n\nSimilar conversion methods are avaiable on many Arrow classes.\n\ntables.get(0).toJSON()\n\n## Slicing Arrow Data\n\nevery10KRow = Array(17) [Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3)]\n\nOur custom arrow data range stepper for sampling data:\n\nrange = ƒ(start, end, step)\n\n### Iterating over Rows and Cells\n\n```js\nfor (let row of dataFrame) {\n  for (let cell of row) {\n    if ( Array.isArray(cell) ) {\n      td = '[' + cell.map((value) => value == null ? 'null' : value).join(', ') + ']';\n    } else if (fields[k] === 'Date') {\n      td = toDate(cell); // convert Apache arrow Timestamp to Date\n    } else {\n      td = cell.toString();\n    }\n    k++;\n  }\n}\n```\n\n\n### Converting Dates\n\nApache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int.\n\n```js\nfunction toDate(timestamp) {\n  return new Date((timestamp[1] * Math.pow(2, 32) + timestamp[0])/1000);\n}\n```\n\n\n### Getting Column Data Stats\n\n\n### Column Data Vectors\n\nApache Arrow stores columns in typed arrays and vectors:\n\nTyped vectors have convinience methods to convert Int32 arrays data to JS values you can work with.\n\nFor example, to get timestamps in milliseconds:\n\ntimestamps = Array(10) [2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01]\n\n### Filtering Timestamped Data\n\n```js\nfunction filterByDate(startDate, endDate) {\n  const dateFilter = arrow.predicate.custom(i => {\n  \tconst arrowDate = table.getColumn('Date').get(i);\n    const date = toDate(arrowDate);\n    return date >= startDate && date <= endDate;\n  }, b => 1);\n\n  const getDate;\n  const results = [];\n  table.filter(dateFilter)\n    .scan(\n      index => {\n        results.push({\n          'date': toDate(getDate(index))\n        });\n      },\n      batch => {\n        getDate = arrow.predicate.col('Date').bind(batch);\n      }\n    );\n\n  return results;\n}\n```\n\nOur custom filter by date method uses custom arrow table predicate filter and scan methods to generate JS friendly data you can map or graph:\n\n\n### Filtering by Days\n\n\n### Mapping Arrow Data\n\n","slug":"docs/developer-guide/tables","title":"Working with Tables"},{"excerpt":"Reading and Writing Arrow Data About RecordBatches Arrow tables are typically split into record batches, allowing them to be incrementally…","rawMarkdownBody":"# Reading and Writing Arrow Data\n\n## About RecordBatches\n\nArrow tables are typically split into record batches, allowing them to be incrementally loaded or written, and naturally the Arrow API provides classes to facilite this reading.\n\n\n## Reading Arrow Data\n\nThe `Table` class provides a simple `Table.from` convenience method for reading an Arrow formatted data file into Arrow data structures:\n\n```\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\nconst arrow = readFileSync('simple.arrow');\nconst table = Table.from([arrow]);\nconsole.log(table.toString());\n```\n\n### Using RecordBatchReader to read from a Data Source\n\nTo read Arrow tables incrementally, you use the `RecordBatchReader` class.\n\nIf you only have one table in your file (the normal case), then you'll only need one `RecordBatchReader`:\n\n```js\nconst reader = await RecordBatchReader.from(fetch(path, {credentials: 'omit'}));\nfor await (const batch of reader) {\n  console.log(batch.length);\n}\n```\n\n### Reading Multiple Tables from a Data Source\n\nThe JavaScript Arrow API supports arrow data streams that contain multiple tables (this is an \"extension\" to the arrow spec). Naturally, each Table comes with its own set of record batches, so to read all batches from all tables in the data source you will need a double loop:\n\n```js\nconst readers = RecordBatchReader.readAll(fetch(path, {credentials: 'omit'}));\nfor await (const reader of readers) {\n  for await (const batch of reader) {\n    console.log(batch.length);\n  }\n}\n```\n\nNote: this code also works if there is only one table in the data source, in which case the outer loop will only execute once.\n\n\n# Writing Arrow Data\n\nThe `RecordStreamWriter` class allows you to write Arrow `Table` and `RecordBatch` instances to a data source.\n\n\n## Using Transform Streams\n\n\n### Connecting to Node Processes\n\nA \n\n\n### Connecting to Python Processes\n\nA more complicated example of using Arrow to go from node -> python -> node:\n\n```js\nconst { AsyncIterable } = require('ix');\nconst { child } = require('event-stream');\nconst { fork } = require('child_process');\nconst { RecordBatchStreamWriter } = require('apache-arrow');\n\nconst compute_degrees_via_gpu_accelerated_sql = ((scriptPath) => (edgeListColumnName) =>\n    spawn('python3', [scriptPath, edgeListColumnName], {\n        env: process.env,\n        stdio: ['pipe', 'pipe', 'inherit']\n    })\n)(require('path').resolve(__dirname, 'compute_degrees.py'));\n\nfunction compute_degrees(colName, recordBatchReaders) {\n    return AsyncIterable\n        .as(recordBatchReaders).mergeAll()\n        .pipe(RecordBatchStreamWriter.throughNode())\n        .pipe(compute_degrees_via_gpu_accelerated_sql(colName));\n}\n\nmodule.exports = compute_degrees;\n\n```\n\nThis example construct pipes of streams of events and that python process just reads from stdin, does a GPU-dataframe operation, and writes the results to stdout. (This example uses Rx/IxJS style functional streaming pipelines).\n\n`compute_degrees_via_gpu_accelerated_sql` returns a node `child_process` that is also a duplex stream, similar to the [`event-stream#child()` method](https://www.npmjs.com/package/event-stream#child-child_process)\n","slug":"docs/developer-guide/reading-and-writing","title":"Reading and Writing Arrow Data"},{"excerpt":"Using with Typescript This documentation does not include advanced type definitions in the interest of simplicity and making the…","rawMarkdownBody":"# Using with Typescript\n\nThis documentation does not include advanced type definitions in the interest of simplicity and making the documentation accessible to more JavaScript developers. If you are working with Typescript in your application and would benefit from documentation that includes the Typescript definitions, you can refer to the auto generated JSDocs for the API.\n\n## Considerations when Using Typescript\n\nTo ensure that type information \"flows\" correctly from the types of function/constructor arguments to the types of returned objects, some special methods are provided (effectively working around limitations in Typescript).\n\nA key example is the availability of static `new()` methods on a number of classes that are intended to be used instead of calling `new` on the constructor. Accordingly, `Table.new()` is an alternative to `new Table()`, that provides stronger type inference on the returned Table.\n\nYou may want to leverage this syntax if your application is written in Typescript.\n","slug":"docs/developer-guide/typescript","title":"Using with Typescript"},{"excerpt":"DataFrame Extends  Methods filter(predicate: Predicate) : FilteredDataFrame Returns: A   which is a subclass of  , allowing you to chain…","rawMarkdownBody":"# DataFrame\n\nExtends `Table`\n\n## Methods\n\n### filter(predicate: Predicate) : FilteredDataFrame\n\nReturns: A `FilteredDataFrame` which is a subclass of `DataFrame`, allowing you to chain additional data frame operations, including applying additional filters.\n\nNote that this operation just registers filter predicates and is this very cheap to call. No actual filtering is done until iteration starts.\n\n### scan(next: Function, bind?: Function)\n\nPerformantly iterates over all non-filtered rows in the data frame.\n\n* `next` `(idx: number, batch: RecordBatch) => void` -\n* `bind` `(batch: RecordBatch) => void` - Optional, typically used to generate high-performance per-batch accessor functions for `next`.\n\n### countBy(name: Col | String) : CountByResult\n\n","slug":"docs/api-reference/data-frame","title":"DataFrame"},{"excerpt":"Column An immutable column data structure consisting of a field (type metadata) and a chunked data array. Usage Copy a column Get a…","rawMarkdownBody":"# Column\n\nAn immutable column data structure consisting of a field (type metadata) and a chunked data array.\n\n## Usage\n\nCopy a column\n```js\nconst typedArray = column.slice();\n```\n\nGet a contiguous typed array from a `Column` (creates a new typed array unless only one chunk)\n```js\nconst typedArray = column.toArray();\n```\n\ncolumns are iterable\n```js\nlet max = column.get(0);\nlet min = max;\nfor (const value of column) {\n  if      (value > max) max = value;\n  else if (value < min) min = value;\n}\n```\n\n\n## Inheritance\n\nColumn extends [`Chunked`](modules/arrow/docs/api-reference/chunked.md)\n\n\n## Fields\n\nIn addition to fields inherited from `Chunked`, Colum also defines\n\n### name : String\n\nThe name of the column (short for `field.name`)\n\n### field : Field\n\nReturns the `Field` instance that describes for the column.\n\n\n## Methods\n\n\n### constructor(field : Field, vectors: Vector, offsets?: Uint32Array)\n\n\n### clone\n\nReturns a new `Column` instance with the same properties.\n\n\n### getChildAt(index : Number) : Vector\n\nReturns the `Vector` that contains the element with \n","slug":"docs/api-reference/column","title":"Column"},{"excerpt":"Apache Arrow JavaScript API Reference Class List TODO - This is a class list from the C++ docs, it has only been partially updated to match…","rawMarkdownBody":"# Apache Arrow JavaScript API Reference\n\n## Class List\n\n> TODO - This is a class list from the C++ docs, it has only been partially updated to match JS API\n\n| Class             | Summary |\n| ---               | ---     |\n| `Array`           | Array base type Immutable data array with some logical type and some length |\n| `ArrayData`       | Mutable container for generic Arrow array data  |\n| `BinaryArray`     | Concrete Array class for variable-size binary data |\n| `BooleanArray`    | Concrete Array class for boolean data  |\n| `Buffer`          | Object containing a pointer to a piece of contiguous  memory with a particular size |\n| `ChunkedArray`    | A data structure managing a list of primitive Arrow arrays logically as one large array |\n| `Column`          | An immutable column data structure consisting of a field (type metadata) and a chunked data array |\n| `Decimal128`      | Represents a signed 128-bit integer in two's  complement |\n| `Decimal128Array` | Concrete Array class for 128-bit decimal data  |\n| `DictionaryArray` | Concrete Array class for dictionary data  |\n| `Field`           | The combination of a field name and data type, with  optional metadata |\n| `FixedSizeBinaryArray` | Concrete Array class for fixed-size  binary data |\n| `FixedWidthType`  | Base class for all fixed-width data types  |\n| `FlatArray`       | Base class for non-nested arrays  |\n| `FloatingPoint`   | Base class for all floating-point data types  |\n| `Int16Type`       | Concrete type class for signed 16-bit integer data  |\n| `Int32Type`       | Concrete type class for signed 32-bit integer data  |\n| `Int64Type`       | Concrete type class for signed 64-bit integer data  |\n| `Int8Type`        | Concrete type class for signed 8-bit integer data  |\n| `Integer`         | Base class for all integral data types  |\n| `ListArray`       | Concrete Array class for list data  |\n| `ListType`        | Concrete type class for list data  |\n| `NestedType`      | |\n| `NullArray`       | Degenerate null type Array  |\n| `NullType`        | Concrete type class for always-null data  |\n| `Number`          | Base class for all numeric data types  |\n| `NumericArray`    | |\n| `PrimitiveArray`  | Base class for arrays of fixed-size logical  types |\n| `RecordBatch`     | Collection of equal-length arrays matching a  particular Schema |\n| `RecordBatchReader` | Abstract interface for reading stream of  record batches |\n| `Schema`          | Sequence of arrow::Field objects describing the  columns of a record batch or table data structure |\n| `Status`          | |\n| `StringArray`     | Concrete Array class for variable-size string ( utf-8) data |\n| `StructArray`     | Concrete Array class for struct data  |\n| `Table`           | Logical table as sequence of chunked arrays  |\n| `TableBatchReader` | Compute a sequence of record batches from a ( possibly chunked) Table |\n| `TimeUnit`        | |\n| `UnionArray`      | Concrete Array class for union data  |\n","slug":"docs/api-reference","title":"Apache Arrow JavaScript API Reference"},{"excerpt":"Field The combination of a field name and data type, with optional metadata. Fields are used to describe the individual constituents of a…","rawMarkdownBody":"# Field\n\nThe combination of a field name and data type, with optional metadata. Fields are used to describe the individual constituents of a nested DataType or a Schema.\n\n\n## Members\n\n### name : String (read only)\n\nThe name of this field.\n\n### type : Type (read only)\n\nThe type of this field.\n\n### nullable : Boolean (read only)\n\nWhether this field can contain `null` values, in addition to values of `Type` (this creates an extra null value map).\n\n### metadata : Object | null (read only)\n\nA field's metadata is represented by a map which holds arbitrary key-value pairs. Returns `null` if no metadata has been set.\n\n### typeId : ?\n\nTBD?\n\n### indices : ?\n\nTBD? Used if data type is a dictionary.\n\n\n## Methods\n\n### constructor(name : String, nullable?: Boolean, metadata?: Object)\n\nCreates an instance of `Field` with parameters initialized as follows:\n\n* `name` - Name of the column\n* `nullable`=`false` - Whether a null-array is maintained.\n* `metadata`=`null` - Map of metadata\n","slug":"docs/api-reference/field","title":"Field"},{"excerpt":"Data Frame Operations Note that   inherits from  , so many examples will apply   methods to  s. Part of the power of data frame operations…","rawMarkdownBody":"# Data Frame Operations\n\nNote that `Table` inherits from `DataFrame`, so many examples will apply `DataFrame` methods to `Table`s.\n\nPart of the power of data frame operations is that they typically do not actually perform any modifications (copying etc) of the underlying data, and ultimately only impact how iteration over that data is done, and what \"view\" of the data is presented. This allows data frame operations to be extremely performant, even when applied on very big (multi-gigabyte) data aset.\n\nMost of the data frame operations do not modify the original `Table` or `DataFrame`, but rather return a new similar object with the new \"iteration constraints\" applied.\n\nReferences:\n* Much of the text in this section is adapted from Brian Hulette's [Introduction to Apache Arrow](https://observablehq.com/@theneuralbit/introduction-to-apache-arrow)\n\n\n## Removing Rows\n\nThe simples way to remove rows from a data frame mey be use `Table.slice(start, end)`. As usual this operation return `Table`/`DataFrame` with iteration constrained to a sub set of the rows in the original frame.\n\n\n## Removing Columns\n\nThe `Table.select(keys: String[])` method drops all columns except the ones that match the supplied `keys`.\n\n\n## Filtering Rows\n\nAnother way to \"remove\" rows fromn data frames to apply filters. Filters effectively remove rows from the data frame that don't fullfill the predicates in the filter.\n\n```js\nconst selectedName = '';\nconst dataFrame = table.filter(arrow.predicate.col('name').eq(selectedName)); // Remove all rows\n```\n\nThe provided predicates allows for the comparison of column values against literals or javascript values (equality, greater or equal than, less or equal than) as well as the creation of composite logical expressions (`and`, `or` and `not`) out of individual column comparisons.\n\nIt is of course also possible to write custom predicates by supplying an arbitrary JavaScript function to filter a row, however performance is usually best when using the built-in comparison predicates.\n\n> Note that calling `filter()` on a `DataFrame` doesn't actually remove any rows from the underlying data store (it just stores the predicates). It's not until you iterate over the date, e.g. by calling `countBy()` or `scan()` that we actually scan through all of the data.\n\n\n## Counting Rows\n\nTo count the number of times different values appear in a table, use `countBy()`.\n\n```js\nconst newTable = table.countBy('column_name');\n```\n\nNote that `countBy()` does not return a modified data frame or table, but instead returns a new `Table` that contains two columns, `value` and `count`. Each distinct value in the specified column in the original table is listed once in `value`, and the corresponding `count` field in the same row indicates how many times it was present in the original table.\n\nNote: Technically a subclass called `CountByResult`).\n\nNote that the results are not sorted.\n\n## Sorting\n\nDataFrames do not currently support sorting. To sort you need to move the data back to JavaScript arrays.\n\n\n## Iterating over a DataFrame (Scanning)\n\nThe `DataFrame.scan()` method lets you define a custom function that will be called for each (non-filtered) record in the `DataFrame`.\n\nNote: For simpler use cases, it is recommended to use the Arrow API provided predicates etc rather than writing a custom scan function, as performance will often be better.\n\n\n### Writing a `next` callback for `scan()`\n\nIn order to be more efficient, Arrow data is broken up into batches of records (which is what makes it possible to do concatenations despite the columnar layout, and `DataFrame.scan()` does not hide this implementation detail from you.\n\n\n### Optimizing `scan()` performance with `bind()` callbacks\n\nIn addition to the `next` callback, you can supply a `bind` function for scan to call each time it starts reading from a new `RecordBatch`. `scan` will call these functions as illustrated in the following pseudo-code:\n\n```js\nfor (batch of batches) {\n  bind(batch);\n  for (idx in batch) {\n    next(idx, batch);\n  }\n}\n```\n\nNote:\n* The idx passed to next only applies to the current RecordBatch, it is not a global index.\n* The current `RecordBatch` is passed to `next`, so it is possible to access data without writing a bind function, but there will be a performance penalty if your data has a lot of batches.\n","slug":"docs/developer-guide/data-frame-operations","title":"Data Frame Operations"},{"excerpt":"Dictionary A   stores index-to-value maps for dictionary encoded columns. Fields indices: V  readonly dictionary: Vector  readonly Static…","rawMarkdownBody":"# Dictionary\n\nA `Dictionary` stores index-to-value maps for dictionary encoded columns.\n\n\n## Fields\n\n### indices: V<TKey> readonly\n### dictionary: Vector<T> readonly\n\n## Static Methods\n\n### Dictionary.from(values: Vector, indices: TKey, keys: ArrayLike<number> | TKey['TArray']) : Dictionary\n\n## Methods\n\n### constructor(data: Data)\n\n### reverseLookup(value: T): number\n\n### getKey(idx: number): TKey['TValue'] | null\n\n### getValue(key: number): T['TValue'] | null\n\n### setKey(idx: number, key: TKey['TValue'] | null): void\n\n### setValue(key: number, value: T['TValue'] | null): void\n","slug":"docs/api-reference/dictionary","title":"Dictionary"},{"excerpt":"Predicates Value Literal Col The Col predicate gets the value of the specified column bind(batch : RecordBatch) : Function Returns a more…","rawMarkdownBody":"# Predicates\n\n\n\n\n## Value\n\n## Literal\n\n## Col\n\nThe Col predicate gets the value of the specified column\n\n### bind(batch : RecordBatch) : Function\n\nReturns a more efficient accessor for the column values in this batch, taking local indices.\n\nNote: These accessors are typically created in the `DataFrame.scan` bind method, and then used in the the `DataFrame.next` method.\n\n## ComparisonPredicate\n\n## And\n\n## Or\n\n## Equals\n\n## LTEq\n\n## GTEq\n\n## Not\n\n## CustomPredicate\n","slug":"docs/api-reference/predicates","title":"Predicates"},{"excerpt":"RecordBatchReader The RecordBatchReader is the IPC reader for reading chunks from a stream or file Usage The JavaScript API supports…","rawMarkdownBody":"# RecordBatchReader\n\nThe RecordBatchReader is the IPC reader for reading chunks from a stream or file\n\n## Usage\n\nThe JavaScript API supports streaming multiple arrow tables over a single socket.\n\nTo read all batches from all tables in a data source:\n\n```js\nconst readers = RecordBatchReader.readAll(fetch(path, {credentials: 'omit'}));\nfor await (const reader of readers) {\n    for await (const batch of reader) {\n        console.log(batch.length);\n    }\n}\n```\n\nIf you only have one table (the normal case), then there'll only be one RecordBatchReader/the outer loop will only execute once. You can also create just one reader via\n\n```js\nconst reader = await RecordBatchReader.from(fetch(path, {credentials: 'omit'}));\n```\n\n\n## Methods\n\n### readAll() : `AsyncIterable<RecordBatchReader>`\n\nReads all batches from all tables in the data source.\n\n\n### from(data : \\*) : RecordBatchFileReader \\| RecordBatchStreamReader\n\n`data`\n* Array\n* fetch response object\n* stream\n\n\nThe `RecordBatchReader.from` method will also detect which physical representation it's working with (Streaming or File), and will return either a `RecordBatchFileReader` or `RecordBatchStreamReader` accordingly.\n\n\n\nRemarks:\n* if you're fetching the table from a node server, make sure the content-type is `application/octet-stream`\n\n\n\n### toNodeStream()\n### pipe()\n\nYou can also turn the RecordBatchReader into a stream\nif you're in node, you can use either toNodeStream() or call the pipe(writable) methods\n\n\n\nin the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call\n\n### toDOMStream() or\n### pipeTo(writable)/pipeThrough(transform)\n\nIn the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call `toDOMStream()` or `pipeTo(writable)`/`pipeThrough(transform)`\n\nYou can also create a transform stream directly, instead of using `RecordBatchReader.from()`\n\nYou can also create a transform stream directly, instead of using `RecordBatchReader.from()`\n\n### throughNode\n### throughDOM\n\nvia `throughNode()` and `throughDOM()` respectively:\n\n1. https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-node-tests.ts#L54\n2. https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-dom-tests.ts#L50\n\nBy default the transform streams will only read one table from the source readable stream and then close, but you can change this behavior by passing `{ autoDestroy: false }` to the transform creation methods\n\n\n## Remarks\n\n* Reading from multiple tables (`readAll()`) is technically an extension in the JavaScript Arrow API compared to the Arrow C++ API. The authors found it was useful to be able to send multiple tables over the same physical socket\nso they built the ability to keep the underlying socket open and read more than one table from a stream.\n* Note that Arrow has two physical representations, one for streaming, and another for random-access so this only applies to the streaming representation.\n* The IPC protocol is that a stream of ordered Messages are consumed atomically. Messages can be of type `Schema`, `DictionaryBatch`, `RecordBatch`, or `Tensor` (which we don't support yet). The Streaming format is just a sequence of messages with Schema first, then `n` `DictionaryBatches`, then `m` `RecordBatches`.\n","slug":"docs/api-reference/record-batch-reader","title":"RecordBatchReader"},{"excerpt":"RecordBatch Overview A Record Batch in Apache Arrow is a collection of equal-length array instances. Usage A record batch can be created…","rawMarkdownBody":"# RecordBatch\n\n## Overview\n\nA Record Batch in Apache Arrow is a collection of equal-length array instances.\n\n## Usage\n\nA record batch can be created from this list of arrays using `RecordBatch.from`:\n```\nconst data = [\n  new Array([1, 2, 3, 4]),\n  new Array(['foo', 'bar', 'baz', None]),\n  new Array([True, None, False, True])\n]\n\nconst recordBatch = RecordBatch.from(arrays);\n```\n\n\n## Inheritance\n\n`RecordBatch` extends [`StructVector`](docs-arrow/api-reference/struct-vector) extends [`BaseVector`](docs-arrow/api-reference/vector)\n\n\n## Members\n\n### schema : Schema (readonly)\n\nReturns the schema of the data in the record batch\n\n### numCols : Number (readonly)\n\nReturns number of fields/columns in the schema (shorthand for `this.schema.fields.length`).\n\n\n## Static Methods\n\n### RecordBatch.from(vectors: Array, names: String[] = []) : RecordBatch\n\nCreates a `RecordBatch`, see `RecordBatch.new()`.\n\n\n### RecordBatch.new(vectors: Array, names: String[] = []) : RecordBatch\n\nCreates new a record batch.\n\nSchema is auto inferred, using names or index positions if `names` are not supplied.\n\n\n## Methods\n\n### constructor(schema: Schema, numRows: Number, childData: (Data | Vector)[])\n\nCreate a new `RecordBatch` instance with `numRows` rows of child data.\n\n* `numRows` - \n* `childData` - \n\n\n### constructor(schema: Schema, data: Data, children?: Vector[])\n\nCreate a new `RecordBatch` instance with `numRows` rows of child data.\n\n### constructor(...args: any[])\n\n### clone(data: Data, children?: Array) : RecordBatch\n\nReturns a newly allocated copy of this `RecordBatch`\n\n### concat(...others: Vector[]) : Table\n\nConcatenates a number of `Vector` instances.\n\n### select(...columnNames: K[]) : RecordBatch\n\nReturn a new `RecordBatch` with a subset of columns.\n","slug":"docs/api-reference/record-batch","title":"RecordBatch"},{"excerpt":"Row A   is an Object that retrieves each value at a certain index across a collection of child Vectors. Rows are returned from the…","rawMarkdownBody":"# Row\n\nA `Row` is an Object that retrieves each value at a certain index across a collection of child Vectors. Rows are returned from the `get()` function of the nested `StructVector` and `MapVector`, as well as `RecordBatch` and `Table`.\n\nA `Row` defines read-only accessors for the indices and (if applicable) names of the child Vectors. For example, given a `StructVector` with the following schema:\n\n```ts\nconst children = [\n    Int32Vector.from([0, 1]),\n    Utf8Vector.from(['foo', 'bar'])\n];\n\nconst type = new Struct<{ id: Int32, value: Utf8 }>([\n    new Field('id', children[0].type),\n    new Field('value', children[1].type)\n]);\n\nconst vector = new StructVector(Data.Struct(type, 0, 2, 0, null, children));\n\nconst row = vector.get(1);\n\nassert((row[0] ===   1  ) && (row.id    === row[0]));\nassert((row[1] === 'bar') && (row.value === row[1]));\n```\n\n`Row` implements the Iterator interface, enumerating each value in order of the child vectors list.\n\nNotes:\n\n- If the Row's parent type is a `Struct`, `Object.getOwnPropertyNames(row)` returns the child vector indices.\n- If the Row's parent type is a `Map`, `Object.getOwnPropertyNames(row)` returns the child vector field names, as defined by the `children` Fields list of the `Map` instance.\n\n## Methods\n\n### [key: string]: T[keyof T]['TValue']\n### [kParent]: MapVector<T> | StructVector<T>\n### [kRowIndex]: number\n### [kLength]: number (readonly)\n### [Symbol.iterator](): IterableIterator<T[keyof T][\"TValue\"]>\n### get(key: K): T[K][\"TValue\"]\n\nReturns the value at the supplied `key`, where `key` is either the integer index of the set of child vectors, or the name of a child Vector\n\n### toJSON(): any\n### toString(): any\n","slug":"docs/api-reference/row","title":"Row"},{"excerpt":"Schema Sequence of arrow::Field objects describing the columns of a record batch or table data structure Accessors fields : Field…","rawMarkdownBody":"# Schema\n\nSequence of arrow::Field objects describing the columns of a record batch or table data structure\n\n\n## Accessors\n\n### fields : Field[] \\(readonly)\n\nReturn the list of fields (columns) in the schema.\n\n### metadata (readonly)\n\nThe custom key-value metadata, if any. metadata may be null.\n\n### dictionaries (readonly)\n\nTBD - List of dictionaries (each dictionary is associated with a column that is dictionary encoded).\n\n### dictionaryFields (readonly)\n\nTBD - List of fields\n\n\n## Methods\n\n### constructor(fields: Field[], metadata?: Object, dictionaries?: Object, dictionaryFields?: Object)\n\nCreates a new schema instance.\n\n\n### select(columnNames) : Schema\n\nReturns a new `Schema` with the Fields indicated by the column names.\n\n\n","slug":"docs/api-reference/schema","title":"Schema"},{"excerpt":"StructVector Methods asMap(keysSorted: boolean = false) TBA","rawMarkdownBody":"# StructVector\n\n\n## Methods\n\n### asMap(keysSorted: boolean = false)\n\nTBA\n","slug":"docs/api-reference/struct-vector","title":"StructVector"},{"excerpt":"Data Untyped storage backing for  . Can be thought of as array of   instances. Also contains slice offset (including null bitmaps). Fields…","rawMarkdownBody":"# Data\n\nUntyped storage backing for `Vector`.\n\nCan be thought of as array of `ArrayBuffer` instances.\n\nAlso contains slice offset (including null bitmaps).\n\n\n## Fields\n\nreadonly type: T;\n\nreadonly length: Number;\n\nreadonly offset: Number;\n\nreadonly stride: Number;\n\nreadonly childData: Data[];\n\nreadonly values: Buffers<T>[BufferType.DATA];\n\nreadonly typeIds: Buffers<T>[BufferType.TYPE];\n\nreadonly nullBitmap: Buffers<T>[BufferType.VALIDITY];\n\nreadonly valueOffsets: Buffers<T>[BufferType.OFFSET];\n\nreadonly ArrayType: any;\n\nreadonly typeId: T['TType'];\n\nreadonly buffers: Buffers<T>;\n\nreadonly nullCount: Number;\n\n\n## Static Methods\n\nConvenience methods for creating Data instances for each of the Arrow Vector types.\n\n### Data.Null<T extends Null>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer) : Data\n\n### Data.Int<T extends Int>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Dictionary<T extends Dictionary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Float<T extends Float>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Bool<T extends Bool>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Decimal<T extends Decimal>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Date<T extends Date_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Time<T extends Time>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Timestamp<T extends Timestamp>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Interval<T extends Interval>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.FixedSizeBinary<T extends FixedSizeBinary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Binary<T extends Binary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data\n\n### Data.Utf8<T extends Utf8>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data\n\n### Data.List<T extends List>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, child: Data<T['valueType']> | Vector<T['valueType']>) : Data\n\n### Data.FixedSizeList<T extends FixedSizeList>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, child: Data | Vector) : Data\n\n### Data.Struct<T extends Struct>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Map<T extends Map_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Union<T extends SparseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Union<T extends DenseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, valueOffsets: ValueOffsetsBuffer, children: (Data | Vector)[]) : Data\n}\n\n\n## Methods\n\n### constructor(type: T, offset: Number, length: Number, nullCount?: Number, buffers?: Partial<Buffers<T>> | Data<T>, childData?: (Data | Vector)[]);\n\n### clone(type: DataType, offset?: Number, length?: Number, nullCount?: Number, buffers?: Buffers<R>, childData?: (Data | Vector)[]) : Data;\n\n### slice(offset: Number, length: Number) : Data\n\n\n","slug":"docs/api-reference/data","title":"Data"},{"excerpt":"Vector Also referred to as  . An abstract base class for vector types. Can support a null map ... TBD Inheritance Fields data: Data…","rawMarkdownBody":"# Vector\n\nAlso referred to as `BaseVector`. An abstract base class for vector types.\n\n* Can support a null map\n* ...\n* TBD\n\n\n## Inheritance\n\n\n## Fields\n\n### data: Data<T> (readonly)\n\nThe underlying Data instance for this Vector.\n\n### numChildren: number (readonly)\n\nThe number of logical Vector children. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map).\n\n### type : T\n\nThe DataType that describes the elements in the Vector\n\n### typeId : T['typeId']\n\nThe `typeId` enum value of the `type` instance\n\n### length : number\n\nNumber of elements in the `Vector`\n\n### offset : number\n\nOffset to the first element in the underlying data.\n\n### stride : number\n\nStride between successive elements in the the underlying data.\n\nThe number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here:\n\n- For `Decimal` types, the stride is 4.\n- For `Date` types, the stride is 1 if the `unit` is DateUnit.DAY, else 2.\n- For `Int`, `Interval`, or `Time` types, the stride is 1 if `bitWidth <= 32`, else 2.\n- For `FixedSizeList` types, the stride is the `listSize` property of the `FixedSizeList` instance.\n- For `FixedSizeBinary` types, the stride is the `byteWidth` property of the `FixedSizeBinary` instance.\n\n### nullCount : Number\n\nNumber of `null` values in this `Vector` instance (`null` values require a null map to be present).\n\n### VectorName : String\n\nReturns the name of the Vector\n\n### ArrayType : TypedArrayConstructor | ArrayConstructor\n\nReturns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType.\n\n### values : T['TArray']\n\nReturns the underlying data buffer of the Vector, if applicable.\n\n### typeIds : Int8Array | null\n\nReturns the underlying typeIds buffer, if the Vector DataType is Union.\n\n### nullBitmap : Uint8Array | null\n\nReturns the underlying validity bitmap buffer, if applicable.\n\nNote: Since the validity bitmap is a Uint8Array of bits, it is _not_ sliced when you call `vector.slice()`. Instead, the `vector.offset` property is updated on the returned Vector. Therefore, you must factor `vector.offset` into the bit position if you wish to slice or read the null positions manually. See the implementation of `BaseVector.isValid()` for an example of how this is done.\n\n### valueOffsets : Int32Array | null\n\nReturns the underlying valueOffsets buffer, if applicable. Only the List, Utf8, Binary, and DenseUnion DataTypes will have valueOffsets.\n\n## Methods\n\n### clone(data: Data<R>, children): Vector<R>\n\nReturns a clone of the current Vector, using the supplied Data and optional children for the new clone. Does not copy any underlying buffers.\n\n### concat(...others: Vector<T>[])\n\nReturns a `Chunked` vector that concatenates this Vector with the supplied other Vectors. Other Vectors must be the same type as this Vector.\n\n\n### slice(begin?: number, end?: number)\n\nReturns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS' `Array.prototype.slice`; they are clamped between 0 and `vector.length` and wrap around when negative, e.g. `slice(-1, 5)` or `slice(5, -1)`\n\n### isValid(index: number): boolean\n\nReturns whether the supplied index is valid in the underlying validity bitmap.\n\n### getChildAt<R extends DataType = any>(index: number): Vector<R> | null\n\nReturns the inner Vector child if the DataType is one of the nested types (Map or Struct).\n\n### toJSON(): any\n\nReturns a dense JS Array of the Vector values, with null sentinels in-place.\n","slug":"docs/api-reference/vector","title":"Vector"},{"excerpt":"Types Objects representing types.","rawMarkdownBody":"# Types\n\nObjects representing types.\n\n","slug":"docs/api-reference/types","title":"Types"},{"excerpt":"Chunked Holds a \"chunked array\" that allows a number of array fragments (represented by   instances) to be treated logically as a single…","rawMarkdownBody":"# Chunked\n\nHolds a \"chunked array\" that allows a number of array fragments (represented by `Vector` instances) to be treated logically as a single vector. `Vector` instances can be concatenated into a `Chunked` without any memory beind copied.\n\n\n## Usage\n\nCreate a new contiguous typed array from a `Chunked` instance (note that this creates a new typed array unless only one chunk)\n\n```js\nconst typedArray = chunked.toArray();\n```\n\nA `Chunked` array supports iteration, random element access and mutation.\n\n\n\n## Inheritance\n\nclass Chunked extends [Vector](docs-arrow/api-reference/vector.md)\n\n\n## Static Methods\n\n### Chunked.flatten(...vectors: Vector[]) : Vector\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nUtility method that flattens a number of `Vector` instances or Arrays of `Vector` instances into a single Array of `Vector` instances. If the incoming Vectors are instances of `Chunked`, the child chunks are extracted and flattened into the resulting Array. Does not mutate or copy data from the Vector instances.\n\nReturns an Array of `Vector` instances.\n\n### Chunked.concat(...chunks: Vector<T>[]): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nConcatenates a number of `Vector` instances of the same type into a single `Chunked` Vector. Returns a new `Chunked` Vector.\n\nNote: This method extracts the inner chunks of any incoming `Chunked` instances, and flattens them into the `chunks` array of the returned `Chunked` Vector.\n\n## Members\n\n### [Symbol.iterator]() : Iterator\n\n`Chunked` arrays are iterable, allowing you to use constructs like `for (const element of chunked)` to iterate over elements. For in-order traversal, this is more performant than random-element access.\n\n### type : T\n\nReturns the DataType instance which determines the type of elements this `Chunked` instance contains. All vector chunks will have this type.\n\n### length: Number  (read-only)\n\nReturns the total number of elements in this `Chunked` instance, representing the length of of all chunks.\n\n### chunks: Vector[]  (read-only)\n\nReturns an array of the `Vector` chunks that hold the elements in this `Chunked` array.\n\n### typeId : TBD  (read-only)\n\nThe `typeId` enum value of the `type` instance\n\n### data : Data  (read-only)\n\nReturns the `Data` instance of the _first_ chunk in the list of inner Vectors.\n\n### ArrayType  (read-only)\n\nReturns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType.\n\n### numChildren  (read-only)\n\nThe number of logical Vector children for the Chunked Vector. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map).\n\n### stride  (read-only)\n\nThe number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here:\n\n- For `Decimal` types, the stride is 4.\n- For `Date` types, the stride is 1 if the `unit` is DateUnit.DAY, else 2.\n- For `Int`, `Interval`, or `Time` types, the stride is 1 if `bitWidth <= 32`, else 2.\n- For `FixedSizeList` types, the stride is the `listSize` property of the `FixedSizeList` instance.\n- For `FixedSizeBinary` types, the stride is the `byteWidth` property of the `FixedSizeBinary` instance.\n\n### nullCount  (read-only)\n\nNumber of null values across all Vector chunks in this chunked array.\n\n### indices : ChunkedKeys<T> | null  (read-only)\n\nIf this is a dictionary encoded column, returns a `Chunked` instance of the indicies of all the inner chunks. Otherwise, returns `null`.\n\n### dictionary: ChunkedDict | null  (read-only)\n\nIf this is a dictionary encoded column, returns the Dictionary.\n\n\n## Methods\n\n### constructor(type : \\*, chunks? : Vector[] = [], offsets? : Number[])\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nCreates a new `Chunked` array instance of the given `type` and optionally initializes it with a list of `Vector` instances.\n\n* `type` - The DataType of the inner chunks\n* `chunks`= - Vectors must all be compatible with `type`.\n* `offsets`= - A Uint32Array of offsets where each inner chunk starts and ends. If not provided, offsets are automatically calculated from the list of chunks.\n\nTBD - Confirm/provide some information on how `offsets` can be used?\n\n\n### clone(chunks? : this.chunks): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nReturns a new `Chunked` instance that is a clone of this instance. Does not copy the actual chunks, so the new `Chunked` instance will reference the same chunks.\n\n\n### concat(...others: Vector<T>[]): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nConcatenates a number of `Vector` instances after the chunks. Returns a new `Chunked` array.\n\nThe supplied `Vector` chunks must be the same DataType as the `Chunked` instance.\n\n### slice(begin?: Number, end?: Number): Chunked\n\nReturns a new chunked array representing the logical array containing the elements within the index range, potentially dropping some chunks at beginning and end.\n\n* `begin`=`0` - The first logical index to be included as index 0 in the new array.\n* `end` - The first logical index to be included as index 0 in the new array. Defaults to the last element in the range.\n\nReturns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS' `Array.prototype.slice`; they are clamped between 0 and `vector.length` and wrap around when negative, e.g. `slice(-1, 5)` or `slice(5, -1)`\n\n\n### getChildAt(index : Number): Chunked | null\n\nIf this `Chunked` Vector's DataType is one of the nested types (Map or Struct), returns a `Chunked` Vector view over all the chunks for the child Vector at `index`.\n\n### search(index: Number): [number, number] | null;\n### search(index: Number, then?: SearchContinuation): ReturnType<N>;\n### search(index: Number, then?: SearchContinuation)\n\nUsing an `index` that is relative to the whole `Chunked` Vector, binary search through the list of inner chunks using supplied \"global\" `index` to find the chunk at that location. Returns the child index of the inner chunk and an element index that has been adjusted to the keyspace of the found inner chunk.\n\n`search()` can be called with only an integer index, in which case a pair of `[chunkIndex, valueIndex]` are returned as a two-element Array:\n\n```ts\nlet chunked = [\n    Int32Vector.from([0, 1, 2, 3]),\n    Int32Vector.from([4, 5, 6, 7, 8])\n].reduce((x, y) => x.concat(y));\n\nlet [chunkIndex, valueIndex] = chunked.search(6)\nassert(chunkIndex === 1)\nassert(valueIndex === 3)\n```\n\nIf `search()` is called with an integer index and a callback, the callback will be invoked with the `Chunked` instance as the first argument, then the `chunkIndex` and `valueIndex` as the second and third arguments:\n\n```ts\nlet getChildValue = (parent, childIndex, valueIndex) =>\n    chunked.chunks[childIndex].get(valueIndex);\nlet childValue = chunked.search(6, (chunked, childIndex, valueIndex) => )\n```\n\n\n### isValid(index: Number): boolean\n\nChecks if the element at `index` in the logical array is valid.\n\nChecks the null map (if present) to determine if the value in the logical `index` is included.\n\n### get(index : Number): T['TValue'] | null\n\nReturns the element at `index` in the logical array, or `null` if no such element exists (e.e.g if `index` is out of range).\n\n### set(index: Number, value: T['TValue'] | null): void\n\nWrites the given `value` at the provided `index`. If the value is null, the null bitmap is updated.\n\n### indexOf(element: Type, offset?: Number): Number\n\nReturns the index of the first occurrence of `element`, or `-1` if the value was not found.\n\n* `offset` - the index to start searching from.\n\n### toArray(): TypedArray\n\nReturns a single contiguous typed array containing data in all the chunks (effectively \"flattening\" the chunks.\n\nNotes:\n* Calling this function creates a new typed array unless there is only one chunk.\n\n\n","slug":"docs/api-reference/chunked","title":"Chunked"},{"excerpt":"Table Logical table as sequence of chunked arrays Overview The JavaScript   class is not part of the Apache Arrow specification as such, but…","rawMarkdownBody":"# Table\n\nLogical table as sequence of chunked arrays\n\n\n## Overview\n\nThe JavaScript `Table` class is not part of the Apache Arrow specification as such, but is rather a tool to allow you to work with multiple record batches and array pieces as a single logical dataset.\n\nAs a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\n\nA Table’s columns are instances of `Column`, which is a container for one or more arrays of the same type.\n\n\n## Usage\n\n`Table.new()` accepts an `Object` of `Columns` or `Vectors`, where the keys will be used as the field names for the `Schema`:\n\n```js\nconst i32s = Int32Vector.from([1, 2, 3]);\nconst f32s = Float32Vector.from([.1, .2, .3]);\nconst table = Table.new({ i32: i32s, f32: f32s });\nassert(table.schema.fields[0].name === 'i32');\n```\n\nIt also accepts a a list of Vectors with an optional list of names or\nFields for the resulting Schema. If the list is omitted or a name is\nmissing, the numeric index of each Vector will be used as the name:\n\n```ts\nconst i32s = Int32Vector.from([1, 2, 3]);\nconst f32s = Float32Vector.from([.1, .2, .3]);\nconst table = Table.new([i32s, f32s], ['i32']);\nassert(table.schema.fields[0].name === 'i32');\nassert(table.schema.fields[1].name === '1');\n```\n\nIf the supplied arguments are `Column` instances, `Table.new` will infer the `Schema` from the `Column`s:\n\n```ts\nconst i32s = Column.new('i32', Int32Vector.from([1, 2, 3]));\nconst f32s = Column.new('f32', Float32Vector.from([.1, .2, .3]));\nconst table = Table.new(i32s, f32s);\nassert(table.schema.fields[0].name === 'i32');\nassert(table.schema.fields[1].name === 'f32');\n```\n\nIf the supplied Vector or Column lengths are unequal, `Table.new` will\nextend the lengths of the shorter Columns, allocating additional bytes\nto represent the additional null slots. The memory required to allocate\nthese additional bitmaps can be computed as:\n\n```ts\nlet additionalBytes = 0;\nfor (let vec in shorter_vectors) {\n additionalBytes += (((longestLength - vec.length) + 63) & ~63) >> 3;\n}\n```\n\nFor example, an additional null bitmap for one million null values would require `125,000` bytes (`((1e6 + 63) & ~63) >> 3`), or approx. `0.11MiB`\n\n\n## Inheritance\n\n`Table` extends Chunked\n\n\n## Static Methods\n\n### Table.empty() : Table\n\nCreates an empty table\n\n### Table.from() : Table\n\nCreates an empty table\n\n### Table.from(source: RecordBatchReader): Table\n### Table.from(source: Promise<RecordBatchReader>): Promise<Table>\n### Table.from(source?: any) : Table\n### Table.fromAsync(source: import('./ipc/reader').FromArgs): Promise<Table>\n### Table.fromVectors(vectors: any[], names?: String[]) : Table\n### Table.fromStruct(struct: Vector) : Table\n\n\n### Table.new(columns: Object)\n### Table.new(...columns)\n### Table.new(vectors: Vector[], names: String[])\n\nType safe constructors. Functionally equivalent to calling `new Table()` with the same arguments, however if using Typescript using the `new` method instead will ensure that types inferred from the arguments \"flow through\" into the return Table type.\n\n\n## Members\n\n### schema (readonly)\n\nThe `Schema` of this table.\n\n\n### length : Number (readonly)\n\nThe number of rows in this table.\n\nTBD: this does not consider filters\n\n\n### chunks : RecordBatch[] \\(readonly)\n\nThe list of chunks in this table.\n\n\n### numCols : Number (readonly)\n\nThe number of columns in this table.\n\n\n## Methods\n\n### constructor(batches: RecordBatch[])\n\nThe schema will be inferred from the record batches.\n\n### constructor(...batches: RecordBatch[])\n\nThe schema will be inferred from the record batches.\n\n### constructor(schema: Schema, batches: RecordBatch[])\n\n### constructor(schema: Schema, ...batches: RecordBatch[])\n\n### constructor(...args: any[])\n\n\nCreate a new `Table` from a collection of `Columns` or `Vectors`, with an optional list of names or `Fields`.\n\nTBD\n\n### clone(chunks?:)\n\nReturns a new copy of this table.\n\n### getColumnAt(index: number): Column | null\n\nGets a column by index.\n\n### getColumn(name: String): Column | null\n\nGets a column by name\n\n### getColumnIndex(name: String) : Number | null\n\nReturns the index of the column with name `name`.\n\n### getChildAt(index: number): Column | null\n\nTBD\n\n### serialize(encoding = 'binary', stream = true) : Uint8Array\n\nReturns a `Uint8Array` that contains an encoding of all the data in the table.\n\nNote: Passing the returned data back into `Table.from()` creates a \"deep clone\" of the table.\n\n\n### count(): number\n\nTBD - Returns the number of elements.\n\n### select(...columnNames: string[]) : Table\n\nReturns a new Table with the specified subset of columns, in the specified order.\n\n### countBy(name : Col | String) : Table\n\nReturns a new Table that contains two columns (`values` and `counts`).\n","slug":"docs/api-reference/table","title":"Table"},{"excerpt":"RecordBatchWriter The   \"serializes\" Arrow Tables (or streams of RecordBatches) to the Arrow File, Stream, or JSON representations for inter…","rawMarkdownBody":"## RecordBatchWriter\n\nThe `RecordBatchWriter` \"serializes\" Arrow Tables (or streams of RecordBatches) to the Arrow File, Stream, or JSON representations for inter-process communication (see also: [Arrow IPC format docs](https://arrow.apache.org/docs/format/IPC.html#streaming-format)).\n\nThe RecordBatchWriter is conceptually a \"transform\" stream that transforms Tables or RecordBatches into binary `Uint8Array` chunks that represent the Arrow IPC messages (`Schema`, `DictionaryBatch`, `RecordBatch`, and in the case of the File format, `Footer` messages).\n\nThese binary chunks are buffered inside the `RecordBatchWriter` instance until they are consumed, typically by piping the RecordBatchWriter instance to a Writable Stream (like a file or socket), enumerating the chunks via async-iteration, or by calling `toUint8Array()` to create a single contiguous buffer of the concatenated results once the desired Tables or RecordBatches have been written.\n\nRecordBatchWriter conforms to the `AsyncIterableIterator` protocol in all environments, and supports two additional stream primitives based on the environment (nodejs or browsers) available at runtime.\n\n* In nodejs, the `RecordBatchWriter` can be converted to a `ReadableStream`, piped to a `WritableStream`, and has a static method that returns a `TransformStream` suitable in chained `pipe` calls.\n* browser environments that support the [DOM/WhatWG Streams Standard](https://github.com/whatwg/streams), corresponding methods exist to convert `RecordBatchWriters` to the DOM `ReadableStream`, `WritableStream`, and `TransformStream` variants.\n\n*Note*: The Arrow JSON representation is not suitable as an IPC mechanism in real-world scenarios. It is used inside the Arrow project as a human-readable debugging tool and for validating interoperability between each language's separate implementation of the Arrow library.\n\n\n## Member Fields\n\nclosed: Promise (readonly)\n\nA Promise which resolves when this `RecordBatchWriter` is closed.\n\n## Static Methods\n\n### RecordBatchWriter.throughNode(options?: Object): DuplexStream\n\nCreates a Node.js `TransformStream` that transforms an input `ReadableStream` of Tables or RecordBatches into a stream of `Uint8Array` Arrow Message chunks.\n\n- `options.autoDestroy`: boolean - (default: `true`) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema.\n- `options.*` - Any Node Duplex stream options can be supplied\n\nReturns: A Node.js Duplex stream\n\nExample:\n\n```js\n\nconst fs = require('fs');\nconst { PassThrough, finished } = require('stream');\nconst { Table, RecordBatchWriter } = require('apache-arrow');\n\nconst table = Table.new({\n    i32: Int32Vector.from([1, 2, 3]),\n    f32: Float32Vector.from([1.0, 1.5, 2.0]),\n});\n\nconst source = new PassThrough({ objectMode: true });\n\nconst result = source\n    .pipe(RecordBatchWriter.throughNode())\n    .pipe(fs.createWriteStream('table.arrow'));\n\nsource.write(table);\nsource.end();\n\nfinished(result, () => console.log('done writing table.arrow'));\n```\n\n### RecordBatchWriter.throughDOM(writableStrategy? : Object, readableStrategy? : Object) : Object\n\nCreates a DOM/WhatWG `ReadableStream`/`WritableStream` pair that together transforms an input `ReadableStream` of Tables or RecordBatches into a stream of `Uint8Array` Arrow Message chunks.\n\n- `options.autoDestroy`: boolean - (default: `true`) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema.\n- `writableStrategy.*`= - Any options for QueuingStrategy\\<RecordBatch\\>\n- `readableStrategy.highWaterMark`? : Number\n- `readableStrategy.size`?: Number\n\nReturns: an object with the following fields:\n\n- `writable`: WritableStream\\<Table | RecordBatch\\>\n- `readable`: ReadableStream\\<Uint8Array\\>\n\n\n\n\n## Methods\n\nconstructor(options? : Object)\n\n* `options.autoDestroy`: boolean -\n\n\n### toString(sync: Boolean): string | Promise<string>\n\n### toUint8Array(sync: Boolean): Uint8Array | Promise<Uint8Array>\n\n\n### writeAll(input: Table | Iterable<RecordBatch>): this\n### writeAll(input: AsyncIterable<RecordBatch>): Promise<this>\n### writeAll(input: PromiseLike<AsyncIterable<RecordBatch>>): Promise<this>\n### writeAll(input: PromiseLike<Table | Iterable<RecordBatch>>): Promise<this>\n\n* [Symbol.asyncIterator](): AsyncByteQueue<Uint8Array>\n\nReturns An async iterator that produces Uint8Arrays.\n\n### toDOMStream(options?: Object): ReadableStream<Uint8Array>\n\nReturns a new DOM/WhatWG stream that can be used to read the Uint8Array chunks produced by the RecordBatchWriter\n\n- `options` - passed through to the DOM ReadableStream constructor, any DOM ReadableStream options.\n\n### toNodeStream(options?: Object): Readable\n\n- `options` - passed through to the Node ReadableStream constructor, any Node ReadableStream options.\n\n### close() : void\n\nClose the RecordBatchWriter. After close is called, no more chunks can be written.\n\n### abort(reason?: any) : void\n### finish() : this\n### reset(sink?: WritableSink<ArrayBufferViewInput>, schema?: Schema | null): this\n\nChange the sink\n\n### write(payload?: Table | RecordBatch | Iterable<Table> | Iterable<RecordBatch> | null): void\n\nWrites a `RecordBatch` or all the RecordBatches from a `Table`.\n\n\n## Remarks\n\n* Just like the `RecordBatchReader`, a `RecordBatchWriter` is a factory base class that returns an instance of the subclass appropriate to the situation: `RecordBatchStreamWriter`, `RecordBatchFileWriter`, `RecordBatchJSONWriter`\n","slug":"docs/api-reference/record-batch-writer","title":" RecordBatchWriter"},{"excerpt":"Types and Vectors Overview Usage Constructing new   instances is done through the static   methods Special Vectors Dictionary Arrays The…","rawMarkdownBody":"# Types and Vectors\n\n## Overview\n\n\n## Usage\n\nConstructing new `Vector` instances is done through the static `from()` methods\n\n\n## Special Vectors\n\n### Dictionary Arrays\n\nThe Dictionary type is a special array type that enables one or more record batches in a file or stream to transmit integer indices referencing a shared dictionary containing the distinct values in the logical array. Later record batches reuse indices in earlier batches and add new ones as needed.\n\nA `Dictionary` is similar to a `factor` in R or a pandas, or \"Categorical\" in Python. It is is often used with strings to save memory and improve performance.\n\n\n### StructVector\n\nHolds nested fields.\n\n\n### Bool Vectors\n\n| Bool Vectors            |\n| ---                     |\n| `BoolVector`            |\n\n\n### Binary Vectors\n\n| Binary Vectors          |\n| ---                     |\n| `BinaryVector`          |\n\n\n## FloatVectors\n\n| Float Vectors           | Backing         | Comments                  |\n| ---                     |\n| `Float16Vector`         | `Uint16Array`   | No native JS 16 bit type, additional methods available |\n| `Float32Vector`         | `Float32Array`  | Holds 32 bit floats       |\n| `Float64Vector`         | `Float64Array`  | Holds 64 bit floats       |\n\n\n### Static FloatVector Methods\n\n### FloatVector.from(data: Uint16Array): Float16Vector;\n### FloatVector.from(data: Float32Array): Float32Vector;\n### FloatVector.from(data: Float64Array): Float64Vector;\n### FloatVector16.from(data: Uint8Array | Iterable<Number>): Float16Vector;\n### FloatVector16.from(data: Uint16Array | Iterable<Number>): Float16Vector;\n### FloatVector32.from(data: Float32['TArray'] | Iterable<Number>): Float32Vector;\n### FloatVector64.from(data: Float64['TArray'] | Iterable<Number>): Float64Vector;\n\n\n## Float16Vector Methods\n\nSince JS doesn't have half floats, `Float16Vector` is backed by a `Uint16Array` integer array. To make it practical to work with these arrays in JS, some extra methods are added.\n\n### toArray() : `Uint16Array`\n\nReturns a zero-copy view of the underlying `Uint16Array` data.\n\nNote: Avoids incurring extra compute or copies if you're calling `toArray()` in order to create a buffer for something like WebGL, but makes it hard to use the returned data as floating point values in JS.\n\n### toFloat32Array() : Float32Array\n\nThis method will convert values to 32 bit floats. Allocates a new Array.\n\n### toFloat64Array() : Float64Array\n\nThis method will convert values to 64 bit floats. Allocates a new Array.\n\n\n## IntVectors\n\n| Int Vectors             | Backing         | Comments                  |\n| ---                     | ---             | ---                       |\n| `Int8Vector`            | `Int8Array`     |                           |\n| `Int16Vector`           | `Int16Array`    |                           |\n| `Int32Vector`           | `Int32Array`    |                           |\n| `Int64Vector`           | `Int32Array`    | 64-bit values stored as pairs of `lo, hi` 32-bit values for engines without BigInt support, extra methods available |\n| `Uint8Vector`           | `Uint8Array`    |                           |\n| `Uint16Vector`          | `Uint16Array `  |                           |\n| `Uint32Vector`          | `Uint32Array `  |                           |\n| `Uint64Vector`          | `Uint32Array`   | 64-bit values stored as pairs of `lo, hi` 32-bit values for engines without BigInt support, extra methods available |\n\n## Int64Vector Methods\n\n### toArray() : `Int32Array`\n\nReturns a zero-copy view of the underlying pairs of `lo, hi` 32-bit values as an `Int32Array`. This Array's length is twice the logical length of the `Int64Vector`.\n\n### toBigInt64Array(): `BigInt64Array`\n\nReturns a zero-copy view of the underlying 64-bit integers as a `BigInt64Array`. This Array has the samne length as the length of the original `Int64Vector`.\n\nNote: as of 03/2019, `BigInt64Array` is only available in v8/Chrome. In JS runtimes without support for `BigInt`, this method throws an unsupported error.\n\n## Uint64Vector Methods\n\n### toArray() : `Uint32Array`\n\nReturns a zero-copy view of the underlying pairs of `lo, hi` 32-bit values as a `Uint32Array`. This Array's length is twice the logical length of the `Uint64Vector`.\n\n### toBigUint64Array(): `BigUint64Array`\n\nReturns a zero-copy view of the underlying 64-bit integers as a `BigUint64Array`. This Array has the samne length as the length of the original `Uint64Vector`.\n\nNote: as of 03/2019, `BigUint64Array` is only available in v8/Chrome. In JS runtimes without support for `BigInt`, this method throws an unsupported error.\n\n## Static IntVector Methods\n\n### IntVector.from(data: Int8Array): Int8Vector;\n### IntVector.from(data: Int16Array): Int16Vector;\n### IntVector.from(data: Int32Array, is64?: boolean): Int32Vector | Int64Vector;\n### IntVector.from(data: Uint8Array): Uint8Vector;\n### IntVector.from(data: Uint16Array): Uint16Vector;\n### IntVector.from(data: Uint32Array, is64?: boolean): Uint32Vector | Uint64Vector;\n\n### Int8Vector.from(this: typeof Int8Vector,   data: Int8Array   | Iterable<number>): Int8Vector;\n### Int16Vector.from(this: typeof Int16Vector,  data: Int16Array  | Iterable<number>): Int16Vector;\n### Int32Vector.from(this: typeof Int32Vector,  data: Int32Array  | Iterable<number>): Int32Vector;\n### Int64Vector.from(this: typeof Int64Vector,  data: Int32Array  | Iterable<number>): Int64Vector;\n### Uint8Vector.from(this: typeof Uint8Vector,  data: Uint8Array  | Iterable<number>): Uint8Vector;\n### Uint16Vector.from(this: typeof Uint16Vector, data: Uint16Array | Iterable<number>): Uint16Vector;\n### Uint32Vector.from(this: typeof Uint32Vector, data: Uint32Array | Iterable<number>): Uint32Vector;\n### Uint64Vector.from(this: typeof Uint64Vector, data: Uint32Array | Iterable<number>): Uint64Vector;\n\n\n## Date Vectors\n\n| Date Vectors            | Backing       |                     |\n| ---                     | ---           | ---                 |\n| `DateDayVector`         | `Int32Array`  |                     |\n| `DateMillisecondVector` | `Int32Array`  | TBD - stride: 2?    |\n","slug":"docs/api-reference/vectors","title":"Types and Vectors"}]}}